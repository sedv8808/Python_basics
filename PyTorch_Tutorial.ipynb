{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PyTorch\n",
    "\n",
    "Torch is a set of moduls and classes that help implement Neural Networks (NN). \n",
    "\n",
    "It uses tensors, which are arrays that we feed into a NN. Torch is like NumPy, however, because we are going to need a lot of features, maybe billions, we will need a GPU to run our NN. Because of this, we do not use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up some tensors\n",
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.,  6.])\n",
      "tensor([9., 5.])\n"
     ]
    }
   ],
   "source": [
    "# Tensors operations\n",
    "print(x * y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the same operations from NumPy in PyTorch; they might have the same or slightly different names. \n",
    "Most significantly, reshape from NumPy, will be view() in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor of zeros with shape (2,4).\n",
    "z = torch.zeros([2,4])\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5258, 0.9811, 0.7569, 0.2252, 0.5493],\n",
       "        [0.2879, 0.3825, 0.5167, 0.1706, 0.2660]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tensor of random values between 0 and 1; with shape (2,5).\n",
    "a = torch.rand([2, 5])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use an array for input into a NN, we need to reshape (\"flatten\") our tensors. For example, the previous tensor would need to become a shape of [1,10] instead of [2, 5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5258, 0.9811, 0.7569, 0.2252, 0.5493, 0.2879, 0.3825, 0.5167, 0.1706,\n",
       "         0.2660]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.view([1,10])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, PyTorch is a library to do array math. However, it does have some extra nice perks (methods) that will help to build a proper NN. \n",
    "\n",
    "I will continue typing how to make a NN using PyTorch.\n",
    "\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "First things first. We need to provide good data for a Neural Network.\n",
    "Otherwise, the output will be wrong.\n",
    "\n",
    "In general, acquiring and preprocessing the data, will require more effort than 'running a NN'. \n",
    "\n",
    "Let's use the MNIST data set to understand how to preprocess Data and run a NN.\n",
    "\n",
    "MNIST set is a set of handwritten digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data \n",
    "train = datasets.MNIST(\"\", train = True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to keep in mind:\n",
    "**Batch size:** How many items at a time we want to pass to our model. This is not a big data set, however deep learning shines on huge datasets. Usually, more samples than you can fit into your computer's memory. Common practice is to feed multiples of 8. \n",
    "We are also aiming to get the model as generalized as possible. Every time we pass a new batch, the model gets optimized. \n",
    "\n",
    "**Shuffle:** We are aiming again at Generalization. If we feed our NN with only zeros first, it will learn to say *\"everything is a zero\"*. But when it learns about ones, *\"everything will be a 1\"*, until it ends up saying *\"everthing is a 9\"*. \n",
    "\n",
    "NNs will always try to find the best and easiest optimization it can. Since we do not want this, we have to put the data little by little and as shuffled as possible. We want the NN to learn general principles rather than figuring out simple tricks.\n",
    "\n",
    "Most of the times, the batching and shuffling has to be preprocessed differently and has to be done on our own. However, since this are methods that exist and the toydata is available, for beginning purposes, it can be used like that.\n",
    "\n",
    "Now, let's iterate over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([3, 0, 7, 3, 1, 0, 3, 7, 0, 3])]\n"
     ]
    }
   ],
   "source": [
    "# Print the first batch of data. Get 10 tensors of the digits and the output.\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the first batch of the data. It has an array of the 10 tensors of one written digits plus a tensor of the actual outputs. \n",
    "\n",
    "Let's see the tensors for the first output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the output is 7\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What shape does x have?\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? What is that 1 doing there? We need a [28,28] shape. Let's transform this using view so we can plot the image using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"clearly\" see a 7!\n",
    "\n",
    "The model will always try to make optimizations by decrease the loss. It will also try to be as simple and easy as possible. \n",
    "\n",
    "Because of this, we need to make sure our data is balanced. If 60% of our data was number 1, the model would adjust weights in a manner it would jump right away to predict a 1. Then, it would get stuck. This would be overfitting our NN. Once it gets stuck, it won't be able to dig itself out.\n",
    "\n",
    "To avoid this, we have to make sure our data is balanced.\n",
    "\n",
    "Let's see if Mnist is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.87%\n",
      "1: 11.24%\n",
      "2: 9.93%\n",
      "3: 10.22%\n",
      "4: 9.74%\n",
      "5: 9.04%\n",
      "6: 9.86%\n",
      "7: 10.44%\n",
      "8: 9.75%\n",
      "9: 9.91%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total +=1\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {round(100*counter_dict[i]/total,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Some things to take into consideration when using PyTorch, is that we are going to use OOP. This means we will be building classes for our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # Inherit methods and attributes\n",
    "        # Define the fully connected layers \n",
    "        self.fc1 = nn.Linear(784, 64) # Input = 784 because our images are 28*28; \n",
    "        self.fc2 = nn.Linear(64, 64) # Output = Target of 3 layers of 64 neurons for hidden layers.\n",
    "        self.fc3 = nn.Linear(64, 64) # We could have done any other output.     \n",
    "        self.fc4 = nn.Linear(64, 10) # Last output only has 10 neurons because we have 10 classes\n",
    "        \n",
    "    # Define path for data through the layers\n",
    "    # Feed forward NN (Data goes forward, does not reverse)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Rectified linear function for activation.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) # Last layer we want a function that can fire only one neuron. We want a prob dist here.\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3184, -2.2770, -2.2262, -2.3206, -2.3031, -2.2491, -2.1601, -2.3999,\n",
       "         -2.4112, -2.3899]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28) # -1; unknown shape\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't initialized weights, first passes are not useful. \n",
    "grad_fn = gradient function; what number is it that we passed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "**Loss:** Measure of how wrong is the model. Our goal is to have a loss decrease. \n",
    "\n",
    "**Optimizer:** Adjust all the possible adjustable weights in such a way to lower the loss slowly over time (learning rate).\n",
    "\n",
    "**Learning Rate:** We are trying to find the minimum of the function. If the rate is too big, we will always miss the minimum. If it is too slow, it might get stuck in a local minimum. We can use a decaying learning rate to avoid the mentioned problems.\n",
    "\n",
    "**Epochs:** How many passes we do through our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3190, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset: # data is a batch of features and labels\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28)) \n",
    "        loss = F.nll_loss(output, y) # Loss metrics; in this case it works because our target is a scalar value.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correct are we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "print(\"Training Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Do the last chunk on the test set\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "print(\"Testing Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most real tasks, this accuracy is... a red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[1].view((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[1].view(-1,28*28))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "With CNNs, we can pass 2D or 3D objects to our neural network without flattening it.\n",
    "\n",
    "An image is a set of pixels. What a convolution will do, is search for features within a subset of these pixels through a window.\n",
    "\n",
    "A 3x3 sized window, or 3x3 pixels convolution is a 3x3 kernel. This window will be slided through all the image and will get a scalar out of each \"slide\". It condeses the image (projection).\n",
    "\n",
    "From this projection, we now get a (max)pooling, which is also like a window that selects the maximum value out of that window. This is drastically simplifying the image. We usually have 3 or 4 convolutional layers.\n",
    "\n",
    "The first layer finds edges curves, simple things(combination of pixels). The second layer, says combinations of curves, edges and finds features out of those: circles, squares. The next one finds featurs of combinations of circles and squares... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further expand my knowledge on CNN, I will work with Microsoft's Cats and Dogs data set.\n",
    "\n",
    "GOAL: Identify if an image has a cat or a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 \n",
    "#from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the Data\n",
    "\n",
    "Some images are landscape, while others are portrait. Furthermore, there are several sizes of images and they are coloured. We need to standardize our data.\n",
    "\n",
    "We will switch the size of the images. Then, if we have a 30x50 image but we want all of them to be 50x50, we will resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = False   # We already have the training data we needed.\n",
    "\n",
    "class DogsVsCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"data/catsanddogsimg/PetImages/Cat\"\n",
    "    DOGS = \"data/catsanddogsimg/PetImages/Dog\"\n",
    "    LABELS = {CATS:0, DOGS:1}\n",
    "    training_data = []\n",
    "    \n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            # print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) # Colour is not relevant in this case; colour does not add another dimension, it adds channels\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]]) # Hot-code vectors with np `eye`\n",
    "                        \n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "\n",
    "                        elif label == self.DOGS:\n",
    "                             self.dogcount += 1\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                \n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print(\"CATS: \", self.catcount)\n",
    "        print(\"DOGS: \", self.dogcount)\n",
    "        \n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVsCats()\n",
    "    dogsvcats.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24946"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 17,  13,  15, ..., 115, 120, 115],\n",
      "       [ 39,  31,  27, ..., 117, 121, 119],\n",
      "       [ 33,  53,  61, ..., 120, 127, 118],\n",
      "       ...,\n",
      "       [ 18,  17,  14, ..., 127, 217, 216],\n",
      "       [ 28,  29,  34, ..., 133, 220, 210],\n",
      "       [140, 129, 140, ..., 212, 211, 209]], dtype=uint8)\n",
      " array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[2][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        # Building the layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 5x5 Kernel\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        # We need to flatten the information.\n",
    "        # Pass data through a linear layer. \n",
    "        # What should be the input?\n",
    "        # Let's create random data to see the shape of what we need. \n",
    "        # That way, we will know the right value to input in our fc1.\n",
    "\n",
    "        x = torch.randn(50,50).view(-1, 1, 50, 50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) # flattening\n",
    "        self.fc2 = nn.Linear(512, 2) # Target is two classes\n",
    "        \n",
    "\n",
    "    def convs(self, x):\n",
    "        '''Fwd method for conv NN'''\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        \n",
    "#         print(x[0].shape)\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = 1) # Activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n",
      "22452 2494\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "# Separate training vs testing data\n",
    "Val_pct = 0.1 # Testing size\n",
    "val_size = int(len(X)*Val_pct)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size :]\n",
    "test_y = y[-val_size :]\n",
    "\n",
    "print(len(train_X), len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a7aef25244d6589a769d14f0c6628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(0.2397, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0,len(train_X), BATCH_SIZE)):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742f454267bf44b4b0c6fbf6a4a8d85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2494.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "round() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-88e0068df997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: round() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if real_class == predicted_class:\n",
    "            correct += 1\n",
    "        total +=1\n",
    "        \n",
    "print(f\"Accuracy:\", round(100*(correct/total), 2), \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
