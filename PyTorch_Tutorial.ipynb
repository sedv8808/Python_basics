{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PyTorch\n",
    "\n",
    "Torch is a set of moduls and classes that help implement Neural Networks (NN). \n",
    "\n",
    "It uses tensors, which are arrays that we feed into a NN. Torch is like NumPy, however, because we are going to need a lot of features, maybe billions, we will need a GPU to run our NN. Because of this, we do not use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up some tensors\n",
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.,  6.])\n",
      "tensor([9., 5.])\n"
     ]
    }
   ],
   "source": [
    "# Tensors operations\n",
    "print(x * y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the same operations from NumPy in PyTorch; they might have the same or slightly different names. \n",
    "Most significantly, reshape from NumPy, will be view() in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor of zeros with shape (2,4).\n",
    "z = torch.zeros([2,4])\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0614, 0.0591, 0.6946, 0.6877, 0.9668],\n",
       "        [0.0151, 0.6563, 0.9949, 0.9918, 0.2348]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tensor of random values between 0 and 1; with shape (2,5).\n",
    "a = torch.rand([2, 5])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use an array for input into a NN, we need to reshape (\"flatten\") our tensors. For example, the previous tensor would need to become a shape of [1,10] instead of [2, 5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0614, 0.0591, 0.6946, 0.6877, 0.9668, 0.0151, 0.6563, 0.9949, 0.9918,\n",
       "         0.2348]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.view([1,10])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, PyTorch is a library to do array math. However, it does have some extra nice perks (methods) that will help to build a proper NN. \n",
    "\n",
    "I will continue typing how to make a NN using PyTorch.\n",
    "\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "First things first. We need to provide good data for a Neural Network.\n",
    "Otherwise, the output will be wrong.\n",
    "\n",
    "In general, acquiring and preprocessing the data, will require more effort than 'running a NN'. \n",
    "\n",
    "Let's use the MNIST data set to understand how to preprocess Data and run a NN.\n",
    "\n",
    "MNIST set is a set of handwritten digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data \n",
    "train = datasets.MNIST(\"\", train = True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to keep in mind:\n",
    "**Batch size:** How many items at a time we want to pass to our model. This is not a big data set, however deep learning shines on huge datasets. Usually, more samples than you can fit into your computer's memory. Common practice is to feed multiples of 8. \n",
    "We are also aiming to get the model as generalized as possible. Every time we pass a new batch, the model gets optimized. \n",
    "\n",
    "**Shuffle:** We are aiming again at Generalization. If we feed our NN with only zeros first, it will learn to say *\"everything is a zero\"*. But when it learns about ones, *\"everything will be a 1\"*, until it ends up saying *\"everthing is a 9\"*. \n",
    "\n",
    "NNs will always try to find the best and easiest optimization it can. Since we do not want this, we have to put the data little by little and as shuffled as possible. We want the NN to learn general principles rather than figuring out simple tricks.\n",
    "\n",
    "Most of the times, the batching and shuffling has to be preprocessed differently and has to be done on our own. However, since this are methods that exist and the toydata is available, for beginning purposes, it can be used like that.\n",
    "\n",
    "Now, let's iterate over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 2, 4, 0, 4, 7, 2, 3, 7, 3])]\n"
     ]
    }
   ],
   "source": [
    "# Print the first batch of data. Get 10 tensors of the digits and the output.\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the first batch of the data. It has an array of the 10 tensors of one written digits plus a tensor of the actual outputs. \n",
    "\n",
    "Let's see the tensors for the first output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the output is 7\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What shape does x have?\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? What is that 1 doing there? We need a [28,28] shape. Let's transform this using view so we can plot the image using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"clearly\" see a 7!\n",
    "\n",
    "The model will always try to make optimizations by decrease the loss. It will also try to be as simple and easy as possible. \n",
    "\n",
    "Because of this, we need to make sure our data is balanced. If 60% of our data was number 1, the model would adjust weights in a manner it would jump right away to predict a 1. Then, it would get stuck. This would be overfitting our NN. Once it gets stuck, it won't be able to dig itself out.\n",
    "\n",
    "To avoid this, we have to make sure our data is balanced.\n",
    "\n",
    "Let's see if Mnist is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.87%\n",
      "1: 11.24%\n",
      "2: 9.93%\n",
      "3: 10.22%\n",
      "4: 9.74%\n",
      "5: 9.04%\n",
      "6: 9.86%\n",
      "7: 10.44%\n",
      "8: 9.75%\n",
      "9: 9.91%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total +=1\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {round(100*counter_dict[i]/total,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Some things to take into consideration when using PyTorch, is that we are going to use OOP. This means we will be building classes for our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # Inherit methods and attributes\n",
    "        # Define the fully connected layers \n",
    "        self.fc1 = nn.Linear(784, 64) # Input = 784 because our images are 28*28; \n",
    "        self.fc2 = nn.Linear(64, 64) # Output = Target of 3 layers of 64 neurons for hidden layers.\n",
    "        self.fc3 = nn.Linear(64, 64) # We could have done any other output.     \n",
    "        self.fc4 = nn.Linear(64, 10) # Last output only has 10 neurons because we have 10 classes\n",
    "        \n",
    "    # Define path for data through the layers\n",
    "    # Feed forward NN (Data goes forward, does not reverse)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Rectified linear function for activation.\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) # Last layer we want a function that can fire only one neuron. We want a prob dist here.\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2517, -2.3727, -2.3062, -2.3601, -2.3024, -2.2610, -2.3470, -2.2570,\n",
       "         -2.3036, -2.2729]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28) # -1; unknown shape\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't initialized weights, first passes are not useful. \n",
    "grad_fn = gradient function; what number is it that we passed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "**Loss:** Measure of how wrong is the model. Our goal is to have a loss decrease. \n",
    "\n",
    "**Optimizer:** Adjust all the possible adjustable weights in such a way to lower the loss slowly over time (learning rate).\n",
    "\n",
    "**Learning Rate:** We are trying to find the minimum of the function. If the rate is too big, we will always miss the minimum. If it is too slow, it might get stuck in a local minimum. We can use a decaying learning rate to avoid the mentioned problems.\n",
    "\n",
    "**Epochs:** How many passes we do through our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8556, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0042, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset: # data is a batch of features and labels\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28)) \n",
    "        loss = F.nll_loss(output, y) # Loss metrics; in this case it works because our target is a scalar value.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correct are we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "print(\"Training Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "# Do the last chunk on the test set\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "print(\"Testing Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most real tasks, this accuracy is... a red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[1].view((28,28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[1].view(-1,28*28))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
